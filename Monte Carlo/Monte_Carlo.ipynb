{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Methods\n",
    "\n",
    "In this notebook the implementation of Monte Carlo algorithm is provided using the Open AI gym Blackjack environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('Blackjack-v1', new_step_api=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every state of the blackjack env is a tuple of \n",
    "* the player's current total points $\\in \\{0,1,\\dots, 31\\}$,\n",
    "* the dealer's face up card $\\in \\{1, \\dots, 10\\}$\n",
    "* Whether the player has a usable ace or not (`no`=0, `yes`=1)\n",
    "\n",
    "The agent can choose from two actions:\n",
    "```\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Monte Carlo Methods\n",
    "\n",
    "Monte Carlo methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to compute an exact result with a deterministic algorithm.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The goal of Monte Carlo methods in reinforcement learning is to estimate the value of a policy. The value of a state $ s $ under a policy $ \\pi $ is defined as:\n",
    "\n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s \\right] $$\n",
    "\n",
    "where:\n",
    "- $ V^\\pi(s) $ is the value of state $ s $ under policy $ \\pi $\n",
    "- $ \\mathbb{E}_\\pi $ denotes the expected value given that the agent follows policy $ \\pi $\n",
    "- $ \\gamma $ is the discount factor\n",
    "- $ R_{t+1} $ is the reward received at time step $ t+1 $\n",
    "- $ S_0 $ is the initial state\n",
    "\n",
    "#### Monte Carlo Prediction\n",
    "\n",
    "Monte Carlo prediction methods estimate the value of a policy by averaging the returns observed after visits to each state. According to the law of large numbers, the average should eventually converge to the expected return. The return $ G_t $ is the total discounted reward from time step $ t $ onward:\n",
    "\n",
    "$$ G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
    "\n",
    "We can calculate the mean of a sequence $x_1, x_2, \\dots$ incrementally:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        \\mu_k & = \\frac{1}{k} \\sum_{i=1}^{k} x_i \\notag\\\\\n",
    "        & = \\frac{1}{k} (x_k + \\sum_{i=1}^{k-1} x_i) \\notag\\\\\n",
    "        & = \\frac{1}{k} (x_k + (k-1) \\mu_{k-1}) \\notag\\\\\n",
    "        & = \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1}) \\notag\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Using that we can calculate the value of state $ V(s) $ as the average of the returns observed after visits to $ s $:\n",
    "\n",
    "$ V(s) \\leftarrow V(s) + \\frac{1}{N(s)} \\left( G_t - V(s) \\right) $, where is the counter for state-visitation.\n",
    "\n",
    "We can use step size $\\alpha$ instead to calculate the running mean.\n",
    "\n",
    "$ V(s) \\leftarrow V(s) + \\alpha \\left( G_t - V(s) \\right) $, where $ \\alpha $ is the step-size parameter.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../resources/first_visit_mc_prediction.png\" alt=\"Alt text\" width=\"500\"/>\n",
    "    <figcaption>Pseudocode for first visit Monte Carlo Prediction</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stochastic_episode(env, probs=[0.8, 0.2]):\n",
    "    \"\"\"\n",
    "    Generate a stochastic episode given a probability distribution over actions.\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        probs = [0.8, 0.2] if state[0] < 18 else [0.2, 0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode.append((next_state, action, reward))\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_prediction(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm.\n",
    "    \"\"\"\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # check the progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(episode)+1)])\n",
    "        # update the sum of the returns, number of visits, and action-value \n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(i+1)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 499000/500000."
     ]
    }
   ],
   "source": [
    "Q = monte_carlo_prediction(env, 500000, generate_stochastic_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Control\n",
    "\n",
    "Monte Carlo control methods aim to find the optimal policy by iteratively improving the policy based on the value estimates. The action-value function $ Q(s, a) $ is defined as the expected return starting from state $ s $, taking action $ a $, and thereafter following policy $ \\pi $:\n",
    "\n",
    "$$ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] $$\n",
    "\n",
    "The policy is then improved by choosing the action that maximizes the action-value function:\n",
    "\n",
    "$$ \\pi(s) = \\arg\\max_a Q(s, a) $$\n",
    "\n",
    "This process is repeated until the policy converges to the optimal policy $ \\pi^* $.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Monte Carlo methods are powerful tools for solving reinforcement learning problems. They are particularly useful when the model of the environment is unknown or difficult to compute. By leveraging random sampling and averaging, Monte Carlo methods provide a way to estimate the value of policies and find the optimal policy through iterative improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
