{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QNetwork class\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer for experience replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MMI5CLJ\\.conda\\envs\\py39_torch\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\MMI5CLJ\\.conda\\envs\\py39_torch\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "buffer_size = 10000\n",
    "batch_size = 128\n",
    "gamma = 0.9\n",
    "lr = 0.0025\n",
    "target_update = 10\n",
    "episodes = 500\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay = 0.99\n",
    "epsilon_min = 0.2\n",
    "tau = 1e-3\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# Define state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Initialize the Q-network and target network\n",
    "q_network = QNetwork(state_size, action_size)\n",
    "target_network = QNetwork(state_size, action_size)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "# # target_network.eval()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "replay_buffer = ReplayBuffer(buffer_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn():\n",
    "\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        episode_reward = 0\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** episode))\n",
    "\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            # Choose action using epsilon-greedy policy\n",
    "            if np.random.random() > epsilon:\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_network(state)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "            else:\n",
    "                action = np.random.randint(action_size)\n",
    "\n",
    "            # Take action and observe next state and reward\n",
    "            next_state, reward, terminated, _ = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            replay_buffer.add((state, action, reward, next_state, terminated))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Train Q-network if replay buffer has enough samples\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                experiences = replay_buffer.sample()\n",
    "                states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "                states = torch.stack(states)\n",
    "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "                rewards_tensor = torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "                next_states = torch.stack(next_states)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                # with torch.no_grad():\n",
    "                max_next_q_values = target_network(next_states).max(dim=1)[0].unsqueeze(1)\n",
    "                targets = rewards_tensor + (gamma * max_next_q_values * (1 - dones))\n",
    "\n",
    "                # Compute current Q-values\n",
    "                q_values = q_network(states).gather(1, actions).squeeze()\n",
    "\n",
    "                # Compute loss and update Q-network\n",
    "                loss = nn.functional.mse_loss(q_values, targets)\n",
    "                # print(f\"Q-values: {q_values}, targets: {targets}, Action: {actions}\")\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        # if episode % target_update == 0:\n",
    "        #     target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        for target_param, local_param in zip(target_network.parameters(), q_network.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        print(f'Episode {episode}, Reward: {episode_reward}, Epsilon: {epsilon}')\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    torch.save(q_network.state_dict(), 'dqn_cartpole.pth')\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MMI5CLJ\\.conda\\envs\\py39_torch\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\MMI5CLJ\\AppData\\Local\\Temp\\ipykernel_14856\\4212424234.py:49: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = nn.functional.mse_loss(q_values, targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 11.0, Epsilon: 1.0\n",
      "Episode 1, Reward: 11.0, Epsilon: 0.99\n",
      "Episode 2, Reward: 20.0, Epsilon: 0.9801\n",
      "Episode 3, Reward: 22.0, Epsilon: 0.970299\n",
      "Episode 4, Reward: 34.0, Epsilon: 0.96059601\n",
      "Episode 5, Reward: 12.0, Epsilon: 0.9509900498999999\n",
      "Episode 6, Reward: 11.0, Epsilon: 0.941480149401\n",
      "Episode 7, Reward: 22.0, Epsilon: 0.9320653479069899\n",
      "Episode 8, Reward: 15.0, Epsilon: 0.9227446944279201\n",
      "Episode 9, Reward: 20.0, Epsilon: 0.9135172474836408\n",
      "Episode 10, Reward: 42.0, Epsilon: 0.9043820750088044\n",
      "Episode 11, Reward: 12.0, Epsilon: 0.8953382542587164\n",
      "Episode 12, Reward: 13.0, Epsilon: 0.8863848717161292\n",
      "Episode 13, Reward: 28.0, Epsilon: 0.8775210229989678\n",
      "Episode 14, Reward: 38.0, Epsilon: 0.8687458127689782\n",
      "Episode 15, Reward: 15.0, Epsilon: 0.8600583546412884\n",
      "Episode 16, Reward: 15.0, Epsilon: 0.8514577710948755\n",
      "Episode 17, Reward: 49.0, Epsilon: 0.8429431933839268\n",
      "Episode 18, Reward: 28.0, Epsilon: 0.8345137614500875\n",
      "Episode 19, Reward: 22.0, Epsilon: 0.8261686238355866\n",
      "Episode 20, Reward: 16.0, Epsilon: 0.8179069375972308\n",
      "Episode 21, Reward: 22.0, Epsilon: 0.8097278682212584\n",
      "Episode 22, Reward: 14.0, Epsilon: 0.8016305895390459\n",
      "Episode 23, Reward: 25.0, Epsilon: 0.7936142836436554\n",
      "Episode 24, Reward: 33.0, Epsilon: 0.7856781408072188\n",
      "Episode 25, Reward: 16.0, Epsilon: 0.7778213593991467\n",
      "Episode 26, Reward: 25.0, Epsilon: 0.7700431458051551\n",
      "Episode 27, Reward: 24.0, Epsilon: 0.7623427143471035\n",
      "Episode 28, Reward: 18.0, Epsilon: 0.7547192872036326\n",
      "Episode 29, Reward: 29.0, Epsilon: 0.7471720943315961\n",
      "Episode 30, Reward: 24.0, Epsilon: 0.7397003733882802\n",
      "Episode 31, Reward: 19.0, Epsilon: 0.7323033696543975\n",
      "Episode 32, Reward: 20.0, Epsilon: 0.7249803359578534\n",
      "Episode 33, Reward: 12.0, Epsilon: 0.7177305325982749\n",
      "Episode 34, Reward: 18.0, Epsilon: 0.7105532272722921\n",
      "Episode 35, Reward: 27.0, Epsilon: 0.7034476949995692\n",
      "Episode 36, Reward: 19.0, Epsilon: 0.6964132180495735\n",
      "Episode 37, Reward: 12.0, Epsilon: 0.6894490858690777\n",
      "Episode 38, Reward: 13.0, Epsilon: 0.682554595010387\n",
      "Episode 39, Reward: 27.0, Epsilon: 0.6757290490602831\n",
      "Episode 40, Reward: 15.0, Epsilon: 0.6689717585696803\n",
      "Episode 41, Reward: 21.0, Epsilon: 0.6622820409839835\n",
      "Episode 42, Reward: 11.0, Epsilon: 0.6556592205741436\n",
      "Episode 43, Reward: 38.0, Epsilon: 0.6491026283684022\n",
      "Episode 44, Reward: 37.0, Epsilon: 0.6426116020847181\n",
      "Episode 45, Reward: 27.0, Epsilon: 0.6361854860638709\n",
      "Episode 46, Reward: 14.0, Epsilon: 0.6298236312032323\n",
      "Episode 47, Reward: 17.0, Epsilon: 0.6235253948912\n",
      "Episode 48, Reward: 10.0, Epsilon: 0.617290140942288\n",
      "Episode 49, Reward: 28.0, Epsilon: 0.611117239532865\n",
      "Episode 50, Reward: 13.0, Epsilon: 0.6050060671375364\n",
      "Episode 51, Reward: 20.0, Epsilon: 0.598956006466161\n",
      "Episode 52, Reward: 13.0, Epsilon: 0.5929664464014994\n",
      "Episode 53, Reward: 34.0, Epsilon: 0.5870367819374844\n",
      "Episode 54, Reward: 20.0, Epsilon: 0.5811664141181095\n",
      "Episode 55, Reward: 15.0, Epsilon: 0.5753547499769285\n",
      "Episode 56, Reward: 35.0, Epsilon: 0.5696012024771592\n",
      "Episode 57, Reward: 16.0, Epsilon: 0.5639051904523875\n",
      "Episode 58, Reward: 16.0, Epsilon: 0.5582661385478637\n",
      "Episode 59, Reward: 48.0, Epsilon: 0.5526834771623851\n",
      "Episode 60, Reward: 37.0, Epsilon: 0.5471566423907612\n",
      "Episode 61, Reward: 12.0, Epsilon: 0.5416850759668536\n",
      "Episode 62, Reward: 14.0, Epsilon: 0.536268225207185\n",
      "Episode 63, Reward: 26.0, Epsilon: 0.5309055429551132\n",
      "Episode 64, Reward: 19.0, Epsilon: 0.525596487525562\n",
      "Episode 65, Reward: 19.0, Epsilon: 0.5203405226503064\n",
      "Episode 66, Reward: 22.0, Epsilon: 0.5151371174238033\n",
      "Episode 67, Reward: 14.0, Epsilon: 0.5099857462495653\n",
      "Episode 68, Reward: 22.0, Epsilon: 0.5048858887870696\n",
      "Episode 69, Reward: 52.0, Epsilon: 0.4998370298991989\n",
      "Episode 70, Reward: 35.0, Epsilon: 0.49483865960020695\n",
      "Episode 71, Reward: 20.0, Epsilon: 0.4898902730042049\n",
      "Episode 72, Reward: 26.0, Epsilon: 0.48499137027416284\n",
      "Episode 73, Reward: 16.0, Epsilon: 0.4801414565714212\n",
      "Episode 74, Reward: 36.0, Epsilon: 0.47534004200570695\n",
      "Episode 75, Reward: 15.0, Epsilon: 0.4705866415856499\n",
      "Episode 76, Reward: 32.0, Epsilon: 0.46588077516979337\n",
      "Episode 77, Reward: 15.0, Epsilon: 0.46122196741809546\n",
      "Episode 78, Reward: 21.0, Epsilon: 0.4566097477439145\n",
      "Episode 79, Reward: 18.0, Epsilon: 0.45204365026647536\n",
      "Episode 80, Reward: 66.0, Epsilon: 0.4475232137638106\n",
      "Episode 81, Reward: 59.0, Epsilon: 0.4430479816261725\n",
      "Episode 82, Reward: 20.0, Epsilon: 0.43861750180991077\n",
      "Episode 83, Reward: 22.0, Epsilon: 0.43423132679181164\n",
      "Episode 84, Reward: 15.0, Epsilon: 0.4298890135238935\n",
      "Episode 85, Reward: 9.0, Epsilon: 0.4255901233886546\n",
      "Episode 86, Reward: 11.0, Epsilon: 0.421334222154768\n",
      "Episode 87, Reward: 13.0, Epsilon: 0.41712087993322033\n",
      "Episode 88, Reward: 19.0, Epsilon: 0.41294967113388814\n",
      "Episode 89, Reward: 16.0, Epsilon: 0.40882017442254925\n",
      "Episode 90, Reward: 43.0, Epsilon: 0.4047319726783238\n",
      "Episode 91, Reward: 31.0, Epsilon: 0.40068465295154054\n",
      "Episode 92, Reward: 15.0, Epsilon: 0.3966778064220251\n",
      "Episode 93, Reward: 32.0, Epsilon: 0.39271102835780486\n",
      "Episode 94, Reward: 30.0, Epsilon: 0.3887839180742268\n",
      "Episode 95, Reward: 54.0, Epsilon: 0.38489607889348454\n",
      "Episode 96, Reward: 25.0, Epsilon: 0.38104711810454966\n",
      "Episode 97, Reward: 29.0, Epsilon: 0.37723664692350417\n",
      "Episode 98, Reward: 9.0, Epsilon: 0.37346428045426916\n",
      "Episode 99, Reward: 15.0, Epsilon: 0.36972963764972644\n",
      "Episode 100, Reward: 24.0, Epsilon: 0.3660323412732292\n",
      "Episode 101, Reward: 20.0, Epsilon: 0.3623720178604969\n",
      "Episode 102, Reward: 8.0, Epsilon: 0.3587482976818919\n",
      "Episode 103, Reward: 12.0, Epsilon: 0.355160814705073\n",
      "Episode 104, Reward: 11.0, Epsilon: 0.35160920655802225\n",
      "Episode 105, Reward: 25.0, Epsilon: 0.348093114492442\n",
      "Episode 106, Reward: 22.0, Epsilon: 0.3446121833475176\n",
      "Episode 107, Reward: 12.0, Epsilon: 0.34116606151404244\n",
      "Episode 108, Reward: 12.0, Epsilon: 0.337754400898902\n",
      "Episode 109, Reward: 15.0, Epsilon: 0.334376856889913\n",
      "Episode 110, Reward: 50.0, Epsilon: 0.33103308832101386\n",
      "Episode 111, Reward: 16.0, Epsilon: 0.3277227574378037\n",
      "Episode 112, Reward: 20.0, Epsilon: 0.3244455298634257\n",
      "Episode 113, Reward: 37.0, Epsilon: 0.3212010745647914\n",
      "Episode 114, Reward: 19.0, Epsilon: 0.3179890638191435\n",
      "Episode 115, Reward: 11.0, Epsilon: 0.31480917318095203\n",
      "Episode 116, Reward: 12.0, Epsilon: 0.3116610814491425\n",
      "Episode 117, Reward: 44.0, Epsilon: 0.30854447063465107\n",
      "Episode 118, Reward: 26.0, Epsilon: 0.3054590259283046\n",
      "Episode 119, Reward: 13.0, Epsilon: 0.30240443566902153\n",
      "Episode 120, Reward: 28.0, Epsilon: 0.2993803913123313\n",
      "Episode 121, Reward: 30.0, Epsilon: 0.296386587399208\n",
      "Episode 122, Reward: 11.0, Epsilon: 0.2934227215252159\n",
      "Episode 123, Reward: 31.0, Epsilon: 0.29048849430996376\n",
      "Episode 124, Reward: 23.0, Epsilon: 0.2875836093668641\n",
      "Episode 125, Reward: 15.0, Epsilon: 0.28470777327319546\n",
      "Episode 126, Reward: 38.0, Epsilon: 0.2818606955404635\n",
      "Episode 127, Reward: 14.0, Epsilon: 0.27904208858505886\n",
      "Episode 128, Reward: 13.0, Epsilon: 0.2762516676992083\n",
      "Episode 129, Reward: 10.0, Epsilon: 0.2734891510222162\n",
      "Episode 130, Reward: 47.0, Epsilon: 0.27075425951199406\n",
      "Episode 131, Reward: 19.0, Epsilon: 0.2680467169168741\n",
      "Episode 132, Reward: 16.0, Epsilon: 0.26536624974770534\n",
      "Episode 133, Reward: 11.0, Epsilon: 0.2627125872502283\n",
      "Episode 134, Reward: 14.0, Epsilon: 0.26008546137772603\n",
      "Episode 135, Reward: 11.0, Epsilon: 0.25748460676394874\n",
      "Episode 136, Reward: 23.0, Epsilon: 0.2549097606963093\n",
      "Episode 137, Reward: 11.0, Epsilon: 0.2523606630893462\n",
      "Episode 138, Reward: 33.0, Epsilon: 0.2498370564584527\n",
      "Episode 139, Reward: 41.0, Epsilon: 0.24733868589386818\n",
      "Episode 140, Reward: 12.0, Epsilon: 0.24486529903492948\n",
      "Episode 141, Reward: 41.0, Epsilon: 0.2424166460445802\n",
      "Episode 142, Reward: 26.0, Epsilon: 0.2399924795841344\n",
      "Episode 143, Reward: 13.0, Epsilon: 0.23759255478829303\n",
      "Episode 144, Reward: 18.0, Epsilon: 0.23521662924041012\n",
      "Episode 145, Reward: 14.0, Epsilon: 0.232864462948006\n",
      "Episode 146, Reward: 12.0, Epsilon: 0.23053581831852593\n",
      "Episode 147, Reward: 13.0, Epsilon: 0.22823046013534068\n",
      "Episode 148, Reward: 33.0, Epsilon: 0.22594815553398728\n",
      "Episode 149, Reward: 10.0, Epsilon: 0.2236886739786474\n",
      "Episode 150, Reward: 22.0, Epsilon: 0.22145178723886091\n",
      "Episode 151, Reward: 24.0, Epsilon: 0.2192372693664723\n",
      "Episode 152, Reward: 22.0, Epsilon: 0.21704489667280757\n",
      "Episode 153, Reward: 44.0, Epsilon: 0.2148744477060795\n",
      "Episode 154, Reward: 14.0, Epsilon: 0.2127257032290187\n",
      "Episode 155, Reward: 14.0, Epsilon: 0.21059844619672852\n",
      "Episode 156, Reward: 17.0, Epsilon: 0.20849246173476124\n",
      "Episode 157, Reward: 27.0, Epsilon: 0.2064075371174136\n",
      "Episode 158, Reward: 77.0, Epsilon: 0.2043434617462395\n",
      "Episode 159, Reward: 13.0, Epsilon: 0.2023000271287771\n",
      "Episode 160, Reward: 48.0, Epsilon: 0.2002770268574893\n",
      "Episode 161, Reward: 32.0, Epsilon: 0.2\n",
      "Episode 162, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 163, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 164, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 165, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 166, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 167, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 168, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 169, Reward: 29.0, Epsilon: 0.2\n",
      "Episode 170, Reward: 34.0, Epsilon: 0.2\n",
      "Episode 171, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 172, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 173, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 174, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 175, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 176, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 177, Reward: 29.0, Epsilon: 0.2\n",
      "Episode 178, Reward: 29.0, Epsilon: 0.2\n",
      "Episode 179, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 180, Reward: 33.0, Epsilon: 0.2\n",
      "Episode 181, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 182, Reward: 33.0, Epsilon: 0.2\n",
      "Episode 183, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 184, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 185, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 186, Reward: 30.0, Epsilon: 0.2\n",
      "Episode 187, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 188, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 189, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 190, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 191, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 192, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 193, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 194, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 195, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 196, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 197, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 198, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 199, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 200, Reward: 32.0, Epsilon: 0.2\n",
      "Episode 201, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 202, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 203, Reward: 32.0, Epsilon: 0.2\n",
      "Episode 204, Reward: 68.0, Epsilon: 0.2\n",
      "Episode 205, Reward: 44.0, Epsilon: 0.2\n",
      "Episode 206, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 207, Reward: 29.0, Epsilon: 0.2\n",
      "Episode 208, Reward: 37.0, Epsilon: 0.2\n",
      "Episode 209, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 210, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 211, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 212, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 213, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 214, Reward: 33.0, Epsilon: 0.2\n",
      "Episode 215, Reward: 38.0, Epsilon: 0.2\n",
      "Episode 216, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 217, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 218, Reward: 33.0, Epsilon: 0.2\n",
      "Episode 219, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 220, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 221, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 222, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 223, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 224, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 225, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 226, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 227, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 228, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 229, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 230, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 231, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 232, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 233, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 234, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 235, Reward: 61.0, Epsilon: 0.2\n",
      "Episode 236, Reward: 31.0, Epsilon: 0.2\n",
      "Episode 237, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 238, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 239, Reward: 42.0, Epsilon: 0.2\n",
      "Episode 240, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 241, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 242, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 243, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 244, Reward: 39.0, Epsilon: 0.2\n",
      "Episode 245, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 246, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 247, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 248, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 249, Reward: 47.0, Epsilon: 0.2\n",
      "Episode 250, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 251, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 252, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 253, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 254, Reward: 35.0, Epsilon: 0.2\n",
      "Episode 255, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 256, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 257, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 258, Reward: 57.0, Epsilon: 0.2\n",
      "Episode 259, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 260, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 261, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 262, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 263, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 264, Reward: 8.0, Epsilon: 0.2\n",
      "Episode 265, Reward: 51.0, Epsilon: 0.2\n",
      "Episode 266, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 267, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 268, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 269, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 270, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 271, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 272, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 273, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 274, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 275, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 276, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 277, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 278, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 279, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 280, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 281, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 282, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 283, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 284, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 285, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 286, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 287, Reward: 25.0, Epsilon: 0.2\n",
      "Episode 288, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 289, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 290, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 291, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 292, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 293, Reward: 47.0, Epsilon: 0.2\n",
      "Episode 294, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 295, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 296, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 297, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 298, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 299, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 300, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 301, Reward: 30.0, Epsilon: 0.2\n",
      "Episode 302, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 303, Reward: 35.0, Epsilon: 0.2\n",
      "Episode 304, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 305, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 306, Reward: 30.0, Epsilon: 0.2\n",
      "Episode 307, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 308, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 309, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 310, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 311, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 312, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 313, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 314, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 315, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 316, Reward: 72.0, Epsilon: 0.2\n",
      "Episode 317, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 318, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 319, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 320, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 321, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 322, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 323, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 324, Reward: 35.0, Epsilon: 0.2\n",
      "Episode 325, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 326, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 327, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 328, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 329, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 330, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 331, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 332, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 333, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 334, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 335, Reward: 48.0, Epsilon: 0.2\n",
      "Episode 336, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 337, Reward: 30.0, Epsilon: 0.2\n",
      "Episode 338, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 339, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 340, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 341, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 342, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 343, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 344, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 345, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 346, Reward: 38.0, Epsilon: 0.2\n",
      "Episode 347, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 348, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 349, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 350, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 351, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 352, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 353, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 354, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 355, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 356, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 357, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 358, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 359, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 360, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 361, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 362, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 363, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 364, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 365, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 366, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 367, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 368, Reward: 35.0, Epsilon: 0.2\n",
      "Episode 369, Reward: 29.0, Epsilon: 0.2\n",
      "Episode 370, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 371, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 372, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 373, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 374, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 375, Reward: 39.0, Epsilon: 0.2\n",
      "Episode 376, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 377, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 378, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 379, Reward: 29.0, Epsilon: 0.2\n",
      "Episode 380, Reward: 31.0, Epsilon: 0.2\n",
      "Episode 381, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 382, Reward: 49.0, Epsilon: 0.2\n",
      "Episode 383, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 384, Reward: 47.0, Epsilon: 0.2\n",
      "Episode 385, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 386, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 387, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 388, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 389, Reward: 54.0, Epsilon: 0.2\n",
      "Episode 390, Reward: 37.0, Epsilon: 0.2\n",
      "Episode 391, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 392, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 393, Reward: 35.0, Epsilon: 0.2\n",
      "Episode 394, Reward: 25.0, Epsilon: 0.2\n",
      "Episode 395, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 396, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 397, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 398, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 399, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 400, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 401, Reward: 31.0, Epsilon: 0.2\n",
      "Episode 402, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 403, Reward: 31.0, Epsilon: 0.2\n",
      "Episode 404, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 405, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 406, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 407, Reward: 43.0, Epsilon: 0.2\n",
      "Episode 408, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 409, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 410, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 411, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 412, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 413, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 414, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 415, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 416, Reward: 40.0, Epsilon: 0.2\n",
      "Episode 417, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 418, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 419, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 420, Reward: 45.0, Epsilon: 0.2\n",
      "Episode 421, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 422, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 423, Reward: 41.0, Epsilon: 0.2\n",
      "Episode 424, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 425, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 426, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 427, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 428, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 429, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 430, Reward: 26.0, Epsilon: 0.2\n",
      "Episode 431, Reward: 71.0, Epsilon: 0.2\n",
      "Episode 432, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 433, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 434, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 435, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 436, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 437, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 438, Reward: 32.0, Epsilon: 0.2\n",
      "Episode 439, Reward: 28.0, Epsilon: 0.2\n",
      "Episode 440, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 441, Reward: 37.0, Epsilon: 0.2\n",
      "Episode 442, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 443, Reward: 8.0, Epsilon: 0.2\n",
      "Episode 444, Reward: 57.0, Epsilon: 0.2\n",
      "Episode 445, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 446, Reward: 37.0, Epsilon: 0.2\n",
      "Episode 447, Reward: 50.0, Epsilon: 0.2\n",
      "Episode 448, Reward: 34.0, Epsilon: 0.2\n",
      "Episode 449, Reward: 36.0, Epsilon: 0.2\n",
      "Episode 450, Reward: 11.0, Epsilon: 0.2\n",
      "Episode 451, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 452, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 453, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 454, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 455, Reward: 17.0, Epsilon: 0.2\n",
      "Episode 456, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 457, Reward: 8.0, Epsilon: 0.2\n",
      "Episode 458, Reward: 21.0, Epsilon: 0.2\n",
      "Episode 459, Reward: 22.0, Epsilon: 0.2\n",
      "Episode 460, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 461, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 462, Reward: 23.0, Epsilon: 0.2\n",
      "Episode 463, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 464, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 465, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 466, Reward: 9.0, Epsilon: 0.2\n",
      "Episode 467, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 468, Reward: 27.0, Epsilon: 0.2\n",
      "Episode 469, Reward: 35.0, Epsilon: 0.2\n",
      "Episode 470, Reward: 10.0, Epsilon: 0.2\n",
      "Episode 471, Reward: 46.0, Epsilon: 0.2\n",
      "Episode 472, Reward: 39.0, Epsilon: 0.2\n",
      "Episode 473, Reward: 58.0, Epsilon: 0.2\n",
      "Episode 474, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 475, Reward: 34.0, Epsilon: 0.2\n",
      "Episode 476, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 477, Reward: 53.0, Epsilon: 0.2\n",
      "Episode 478, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 479, Reward: 14.0, Epsilon: 0.2\n",
      "Episode 480, Reward: 24.0, Epsilon: 0.2\n",
      "Episode 481, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 482, Reward: 43.0, Epsilon: 0.2\n",
      "Episode 483, Reward: 25.0, Epsilon: 0.2\n",
      "Episode 484, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 485, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 486, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 487, Reward: 59.0, Epsilon: 0.2\n",
      "Episode 488, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 489, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 490, Reward: 18.0, Epsilon: 0.2\n",
      "Episode 491, Reward: 20.0, Epsilon: 0.2\n",
      "Episode 492, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 493, Reward: 19.0, Epsilon: 0.2\n",
      "Episode 494, Reward: 15.0, Epsilon: 0.2\n",
      "Episode 495, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 496, Reward: 16.0, Epsilon: 0.2\n",
      "Episode 497, Reward: 13.0, Epsilon: 0.2\n",
      "Episode 498, Reward: 12.0, Epsilon: 0.2\n",
      "Episode 499, Reward: 22.0, Epsilon: 0.2\n"
     ]
    }
   ],
   "source": [
    "rewards = train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "q_network = QNetwork(state_size=state_size, action_size=action_size)\n",
    "q_network.load_state_dict(torch.load('dqn_cartpole_best.pth'))\n",
    "q_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_cartpole(render_speed=0.05):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    terminated = False\n",
    "    total_reward = 0\n",
    "    while not terminated:\n",
    "        env.render()\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            print(f\"State: {state}, Q-values: {q_values}, Action: {action}\")\n",
    "        next_state, reward, terminated, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        total_reward += reward\n",
    "\n",
    "        time.sleep(render_speed)\n",
    "\n",
    "    print('Total reward:', total_reward)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([0.0089, 0.0447, 0.0237, 0.0168]), Q-values: tensor([1.2048, 1.2041]), Action: 0\n",
      "State: tensor([ 0.0098, -0.1507,  0.0240,  0.3169]), Q-values: tensor([1.2029, 1.2044]), Action: 1\n",
      "State: tensor([0.0068, 0.0441, 0.0303, 0.0319]), Q-values: tensor([1.2048, 1.2041]), Action: 0\n",
      "State: tensor([ 0.0077, -0.1515,  0.0310,  0.3340]), Q-values: tensor([1.2029, 1.2044]), Action: 1\n",
      "State: tensor([0.0046, 0.0432, 0.0376, 0.0512]), Q-values: tensor([1.2047, 1.2042]), Action: 0\n",
      "State: tensor([ 0.0055, -0.1525,  0.0387,  0.3555]), Q-values: tensor([1.2029, 1.2045]), Action: 1\n",
      "State: tensor([0.0024, 0.0421, 0.0458, 0.0753]), Q-values: tensor([1.2046, 1.2042]), Action: 0\n",
      "State: tensor([ 0.0033, -0.1537,  0.0473,  0.3820]), Q-values: tensor([1.2029, 1.2045]), Action: 1\n",
      "State: tensor([0.0002, 0.0408, 0.0549, 0.1046]), Q-values: tensor([1.2046, 1.2042]), Action: 0\n",
      "State: tensor([ 0.0010, -0.1551,  0.0570,  0.4141]), Q-values: tensor([1.2029, 1.2045]), Action: 1\n",
      "State: tensor([-0.0021,  0.0392,  0.0653,  0.1400]), Q-values: tensor([1.2047, 1.2042]), Action: 0\n",
      "State: tensor([-0.0013, -0.1568,  0.0681,  0.4525]), Q-values: tensor([1.2026, 1.2042]), Action: 1\n",
      "State: tensor([-0.0044,  0.0373,  0.0772,  0.1821]), Q-values: tensor([1.2047, 1.2043]), Action: 0\n",
      "State: tensor([-0.0037, -0.1589,  0.0808,  0.4980]), Q-values: tensor([1.2021, 1.2038]), Action: 1\n",
      "State: tensor([-0.0069,  0.0350,  0.0908,  0.2319]), Q-values: tensor([1.2047, 1.2043]), Action: 0\n",
      "State: tensor([-0.0062, -0.1613,  0.0954,  0.5518]), Q-values: tensor([1.2017, 1.2034]), Action: 1\n",
      "State: tensor([-0.0094,  0.0324,  0.1064,  0.2906]), Q-values: tensor([1.2046, 1.2042]), Action: 0\n",
      "State: tensor([-0.0087, -0.1641,  0.1122,  0.6148]), Q-values: tensor([1.2019, 1.2036]), Action: 1\n",
      "State: tensor([-0.0120,  0.0293,  0.1245,  0.3595]), Q-values: tensor([1.2038, 1.2036]), Action: 0\n",
      "State: tensor([-0.0114, -0.1673,  0.1317,  0.6887]), Q-values: tensor([1.2024, 1.2042]), Action: 1\n",
      "State: tensor([-0.0148,  0.0257,  0.1455,  0.4402]), Q-values: tensor([1.2030, 1.2029]), Action: 0\n",
      "State: tensor([-0.0143, -0.1711,  0.1543,  0.7750]), Q-values: tensor([1.2020, 1.2039]), Action: 1\n",
      "State: tensor([-0.0177,  0.0216,  0.1698,  0.5346]), Q-values: tensor([1.2017, 1.2018]), Action: 1\n",
      "State: tensor([-0.0173,  0.2140,  0.1805,  0.2999]), Q-values: tensor([1.2047, 1.2026]), Action: 0\n",
      "State: tensor([-0.0130,  0.0168,  0.1865,  0.6436]), Q-values: tensor([1.1992, 1.1996]), Action: 1\n",
      "State: tensor([-0.0126,  0.2089,  0.1994,  0.4150]), Q-values: tensor([1.2018, 1.1999]), Action: 0\n",
      "State: tensor([-0.0085,  0.0116,  0.2077,  0.7633]), Q-values: tensor([1.1983, 1.1982]), Action: 0\n",
      "Total reward: 27.0\n"
     ]
    }
   ],
   "source": [
    "render_cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
